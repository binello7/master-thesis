\chapter{Case studies}
\label{chp:case_studies}
%-------------------------------------------------------------------------------

\noindent{\LARGE\textbf{Case study 1}}
%===============================================================================
\section{A mechanistic emulator: fitting the \emph{weir equation}}
\label{sec:mechanistic_emulator}
%===============================================================================

As a first case study to apply the acquired knowledge, it was decided to build an utterly mechanistic emulator.
The main goal of this case study is to try to fit the \emph{weir equation} to simulated data instead of to experimental data.
Here we essentially rediscover the way science has always been done: by observing, measuring and trying to discover mathematical relationships.
The most significant difference is the fact that the experimental set-up is completely computer-built.\\

The \emph{weir equation} is a partially theoretical equation that provides an estimate of the discharge $Q$ over a weir as a function of the water depth above the weir itself ($h_w$).
The equation can be derived from the Bernoulli equation under certain assumptions \autocite{bos_discharge_1989} and can be found in many different forms.
The form used here is the one proposed by \emph{Francis} \autocite{walcott_weir_1907}:

\begin{equation}\label{eq:weir_eq}
  Q = \textcolor{red}{C} \cdot L \cdot h_w^{\textcolor{red}{a}}\, , \quad \mbox{usu. } \textcolor{red}{a = 3/2}
\end{equation}

\noindent Where the empirical coefficient $C$ corrects for the assumption of absence of viscous effects and uniform velocity distribution, $L$ is the length of the weir (perpendicular to the channel) and $h_w$ is the water height above the weir.

In addition to the parametric model (\textit{weir equation}), two non-parametric local techniques, namely \emph{linear interpolation} and \emph{cubic spline interpolation}, were used to intrapolate between the simulated data points.\\


%-------------------------------------------------------------------------------
\subsection{Brief experiment description}\label{sec:cs1_brief_description}
%-------------------------------------------------------------------------------

For this experiment, simulations were run in a flat rectangular channel.
A weir with a trapezoidal cross section is located at the channel half-length.
As initial condition the upstream side of the weir was filled with water up to the weir crest.
At the domain top boundary a constant inflow discharge was set, while at the bottom boundary water could freely outflow.
As the simulation runs, the inflow water flows down the channel, overflows the weir and leaves the domain through the lower boundary (see Fig.~\ref{fig:channel}).
After some time the simulation reaches the \emph{steady-state} conditions: inflow, discharge over the weir and outflow have the same magnitude and the water height above the weir has stabilized.
At this point the value $h_w$ was extracted and was paired with the discharge value $Q$ generating it.
\num{25} experiments were conducted with $Q$ linearly spaced in the range \SIrange{0.1}{10}{\cubic\meter\per\second}.
All simulated pairs $(Q, h_w)$ constitute an \emph{inputs-output} sets to which the weir equation was fitted.\\

In order to ensure the convergence of the simulator solution, and therefore the quality of the experimental results, a \emph{grid convergence study} was performed prior to the experiment.
For this, the simulation with the highest discharge was repeated with successive grid refinements.
The value of the variable of interest, water height, was then compared between the different simulations to find at which grid resolution the solution stabilizes.


%-------------------------------------------------------------------------------
\subsection{Material and methods}
%-------------------------------------------------------------------------------
%...............................................................................
\subsubsection{Generating the topography}
%...............................................................................
The topography used for running the simulations was generated in \citetalias{octave_community_gnu_2018} and represents a flat channel of \SI{40}{\meter} length with a weir placed at its midpoint, at \SI{20}{\meter} distance from the top boundary.
The channel cross section is a rectangle of \SI{4}{\meter} width and the weir has a trapezoidal shape.
Fig.~\ref{fig:weir_scheme} shows the geometry of the channel.
The weir has a crest width of \SI{2}{\meter} and therefore belongs to the \emph{broad-crested} class. The $C$ coefficient for broad-crested weirs with vertical walls and \SI{2}{\meter} crest width varies between \num{1.36} and \num{1.53}, depending on the water height $h_w$ \autocite{brown_urban_2009}.
We therefore expect a value close to this for our experiment.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/weir_scheme.png}
  \caption{Geometry of the weir used for the experiments.}
  \label{fig:weir_scheme}
\end{figure}

%...............................................................................
\subsubsection{Setting up the simulations}
%...............................................................................
After generating the topography to be used for the experiment, the simulation parameters were defined.

The simulation was done with no rain and no infiltration.
This way, water just enters the system through the top boundary and leaves it through the bottom one.
For this a \emph{Neumann} boundary condition was chosen at the bottom, while an \emph{imposed discharge} boundary condition was set at the inflow boundary.
This boundary condition needs two values to be specified: the magnitude of the inflow and an imposed water height, which is used under supercritical flow conditions.
The \num{25} inflow discharges used were obtained with the following \citetalias{octave_community_gnu_2018} code:\\

\inputminted[
  fontsize=\footnotesize,
  firstline=4,
  lastline=5,
  numbersep=2pt,
  gobble=0,
  frame=none,
  bgcolor=light-gray,
  framesep=10mm
]{octave}{code.m}\\

\noindent where, by definition within \citetalias{delestre_fullswof:_2017}, the minus sign indicates that water steadily \emph{enters} the domain.
The imposed height was set to \SI{3}{\m}, corresponding to the weir height.

The channel topography presents no banks.
Wall boundary conditions were set for the lateral boundaries in order to produce the rectangular cross section.
The wall boundaries extend infinitely high, preventing the water from overflowing.
Simulations were run for $t_{max} = \SI{200}{\s}$ and \num{200} intermediate states were saved, giving a time resolution of the simulation output of \SI{1}{\s}.
For the channel a uniform Manning roughness coefficient of \SI{0.03}{\s\per\m\tothe{1/3}} was used.

Since infiltration was not active for the simulations, all of the parameters relative to the soil model did not have to be set.
The number of cells in x, and in y direction were defined according to the grid convergence study explained here below.


%...............................................................................
\subsubsection{Conducting the grid convergence study}
%...............................................................................
The chosen grid resolution influences accuracy of simulation results as well as simulation runtime.
In order to find the appropriate grid resolution, a grid convergence study was performed.
Successive refinements were tested in order to find out at which resolution the solution converges.
A criterion has to be established, in order to decide when the solution has satisfactorily converged.
If the Grid Convergence Index (GCI) is used, then a GCI lower than \SI{5}{\percent} for the last refinement is usually taken as a criterion \autocite{ali_grid_2009}.

For the mesh study \num{7} simulations were run by varying the grid resolution ($Nx$, $Ny$) only.
Topography and parameters used are those mentioned in the two preceding sections.
Squared cells were used and the inflow discharge was set to the highest discharge value of the experiment, namely \SI{10}{\cubic\meter\per\second}.
This discharge is the one generating the highest flow velocities.
If convergence under these conditions is reached, then convergence for lower discharges is also assured.
Tab.~\ref{tab:mesh_study} summarizes the simulations' main characteristics.

\begin{table}[h]
  \centering
  \caption{Summary of simulations runtime, grid resolution and other related parameters for the mesh convergence study.}
  \label{tab:mesh_study}
  \begin{tabular}{crrcccccr}
    \toprule
    \# & \multicolumn{1}{c}{Nx} & \multicolumn{1}{c}{Ny} & \multicolumn{1}{c}{Lx $[\si{\m}]$} & \multicolumn{1}{c}{Ly $[\si{\m}]$} & \multicolumn{1}{c}{dx $[\si{\m}]$} & \multicolumn{1}{c}{dy $[\si{\m}]$} & \multicolumn{1}{c}{Q $[\si{\cubic\m\per\s}]$} & \multicolumn{1}{c}{runtime $[\si{\s}]$} \\
    \midrule
    1  & 2             & 20            & 4               & 40          & 2.00        & 2.00        & 10                      & 0.00 \\
    2  & 4             & 40            & 4               & 40          & 1.00        & 1.00        & 10                      & 0.07 \\
    3  & 8             & 80            & 4               & 40          & 0.50        & 0.50        & 10                      & 0.60 \\
    4  & 20            & 200           & 4               & 40          & 0.20        & 0.20        & 10                      & 9.00 \\
    5  & 40            & 400           & 4               & 40          & 0.10        & 0.10        & 10                      & 69.60 \\
    6  & 80            & 800           & 4               & 40          & 0.05        & 0.05        & 10                      & 537.72 \\
    7  & 100           & 1000          & 4               & 40          & 0.04        & 0.04        & 10                      & 1048.40 \\
    \bottomrule
  \end{tabular}
\end{table}

Fig.~\ref{fig:water_profiles} shows the free surface profiles of the \num{7} simulations at steady-state conditions along the channel axis.
It can be noticed that for $Ny =$ \numlist{20;40;80} the water depth is visibly higher than for further refinements, and its shape quite varying.
This means that the solution has not converged yet.
Results for $Ny =$ \numlist{400;800;1000} are very close, since the lines are almost overlapping.
$Ny =$ \num{200} shows a similar profile shape, but the height is still diverging significantly from the successive refinements.
To better observe the variations between the different refinements, the $h$ value convergence at the weir crest was studied.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/water_profiles.png}
  \caption{Longitudinal free surface profiles at steady-state conditions for the \num{7} different grid resolutions. Convergence of the solution for finer grids can be observed.}
  \label{fig:water_profiles}
\end{figure}

In Fig.~\ref{fig:diff_center} the percent variation in the value of $h$ can be observed.
Measured absolute $h$ values at the weir center can be observed in Fig.~\ref{fig:convergence_center} in the Appendix. 
At the \nth{4} refinement the percent variation is smaller than \SI{0.01}{\percent}.
This variation is small enough to assert that the solution has satisfactory converged.
For the experiment it was therefore decided to use $(Nx, Ny) = (\num{40}, \num{400})$ corresponding to a grid resolution of \SI{0.10}{\m} in both x and y directions. The simulation runtime of $\approx \SI{1}{\hour} \SI{10}{\minute}$, required at this resolution is still quite acceptable.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/diff_center.png}
  \caption{Percent variation of the measured $h$ at the weir midpoint between the given $dy$ and the previous one.}
  \label{fig:diff_center}
\end{figure}


%...............................................................................
\subsubsection{Extracting the dataset}
%...............................................................................
The weir equation computes the discharge over the weir as a function of the water height $h_w$ under steady-state conditions.
These are reached after $\approx \SI{50}{\s}$ of simulation.
After this time, small oscillations of the water surface are still present.
To remove these, a time averaged free surface from $t = \SI{100}{\s}$ to $t = \SI{200}{\s}$ was computed.
Fig.~\ref{fig:free_surfaces} presents the free surface profiles of the \num{25} experiments at steady-state conditions.
The three lowest profiles correspond to the simulation runs with the three lowest inflow discharges. 
As initial condition the weir's upstream side of the channel was filled with water to the weir's height.
The fact that these three profiles are lower than the initial condition is due to a problem with \citetalias{delestre_fullswof:_2017} that lost water from the top boundary.
These simulations were discarded for the continuation of the experiment.
The height value for the remaining simulations was extracted \SI{1.2}{\m} before the weir base, to avoid observations in the acceleration zone, happening in proximity of the weir crest.
At this point a space average over the channel breadth was taken.
The procedure was repeated for all of the experiments.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/free_surfaces.png}
  \caption{Free surface profiles along the channel axis for the \num{25} experiments.}
  \label{fig:free_surfaces}
\end{figure}


%...............................................................................
\subsubsection{Fitting the data}
%...............................................................................
The weir equation (Eq.~\ref{eq:weir_eq}) was fitted to the dataset previously extracted using linear regression.
As the original weir equation is non-linear with respect to the parameters, it was thus linearized by taking the logarithm:

\begin{equation}
  \log(Q) = \log(C) + \log(L) + a \cdot \log(h_w)
\end{equation}

\noindent The values of the parameters $a$ and $C$ were determined from the linearized model.
The performance of the fitted weir equation was compared with the relationship obtained using linear and cubic spline interpolation.

%...............................................................................
\subsubsection{Computing the error}\label{sec:compute_error}
%...............................................................................
To evaluate the performance of the three models, an increasing number of observations were progressively removed, the models trained on the remaining observations and evaluated on the leaved-out observations.
For an amount $k$ of observations removed, all possible combinations were tested. The number of model evaluations follows:

\begin{equation}
  \binom{n}{k} = \frac{n!}{k!\left(n-k\right)!}
\end{equation}

\noindent Since this number grows very fast a subset of the initial dataset was used.
This can be found in Tab.~\ref{tab:dataset_error} in the Appendix.
The first and last points of the dataset were kept during all the \emph{cross-validation} experiment in order to avoid wild extrapolation at the boundaries of the inputs space.
From the remaining \num{12} points all possible combinations of \num{1} to \num{10} points were removed.
The average root mean squared error (RMSE) observed for the removal of \num{1} up to \num{10} points is displayed in Fig~.\ref{fig:fitting_errors}.

%-------------------------------------------------------------------------------
\subsection{Results and Discussion}
%-------------------------------------------------------------------------------

In Fig.~\ref{fig:simulations_results} the $(Q, h_w)$ pairs extracted from the simulations are plotted.
The gap in the lower part of the plot is due to the data that had to be discarded.
The displayed data points represent the input-output set which was used to fit the weir equation and to apply the  linear and cubic spline interpolation.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/simulations_results.png}
  \caption{Plot of the $(Q, h_w)$ pairs extracted from the simulations. Points corresponding to unsuccessful simulations were discarded, causing the gap visible in the bottom left part of the plot.}
  \label{fig:simulations_results}
\end{figure}

The fit of the different models to the input-output sets is displayed in Fig.~\ref{fig:fitting_results}.
It can be observed that the linear interpolation fails to model the initial curvature which is instead captured by the fitting of the weir equation and the cubic spline interpolation.
This is due to two main reasons. 
First, the density of data points in this region is low---the linear interpolation simply joins the two nearest data points with a segment---and on such a long distance it can diverge considerably from non-linear models.
Secondly, this region of the input space presents a pronounced curvature, that makes the linear interpolation diverge faster.

With the remaining data points, all models seem to perform well: the three lines are almost perfectly overlaying, meaning that all give very similar results.
Nevertheless, slight divergences can be observed in proximity of the data points, especially between the weir equation and the two interpolation curves. This is due to the fact that the two interpolation models \emph{interpolate} exactly the observations, whereas the curve obtained by fitting the weir equation (which represents the best linear fit to the observations minimizing the least-squares-error), does not necessarily pass exactly through the data points.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/fitting_results.png}
  \caption{Fit of the three models to the simulated dataset. In blue the weir equation with $C = \num{1.65}$ and $a = \num{1.59}$, in red the linear interpolation through the observations and in green the cubic spline interpolation.}
  \label{fig:fitting_results}
\end{figure}

For the fitting of the weir equation the values $C = 1.65$ and $a = 1.59$ were found.
As mentioned in Sec.~\ref{sec:cs1_brief_description}, for a weir of \SI{2}{\m} breadth and vertical walls, $C$-values in the range $[\numrange{1.36}{1.53}]$ are expected.
The value obtained for $C$ assumes that $C$ does not vary with $h_w$.
Moreover, the weir used for the experiment has a trapezoidal cross section instead of a rectangular one.
\cite{tracy_discharge_1957} investigated how the coefficient $C$ varies with different weir shapes.
For a weir with sloping walls the $C$ coefficient increases, because the head losses are lower in comparison to one with vertical walls.
The difference in the $C$ coefficient observed here can be explained by this phenomenon.\\

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/fitting_errors.png}
  \caption{Mean RMSE of the three different models as a function of the number of observations used to train the models. It can be observed, that the RMSE of the cubic spline decreases exponentially by increasing the number of observations}
  \label{fig:fitting_errors}
\end{figure}

Fig.~\ref{fig:fitting_errors} displays the RMSE obtained by cross-validation as explained in Sec.~\ref{sec:compute_error}.
No matter how many observations are used, the linear interpolation always performs worst.
By increasing the number of observations its performance improves quite rapidly, but not enough to reach that of the other two models, because the process generating the observations does not have a linear behaviour.
 
The results for the cubic splines interpolation confirm what was already observed in Fig.~\ref{fig:fitting_results}:  cubic splines interpolation seems to mimic in an accurate manner the process originating the observations.  This is explainable by the high flexibility of cubic splines interpolation method, which typically shows very small errors when fitted with many observations. However, by reducing the number of observations available for interpolation, the error of this model increases exponentially. Despite this, on average if more than 5 observations are available, cubic splines interpolation outperforms the other methods.

On the other hand, the regression line obtained using the weir equation model shows only a slight decrease in accuracy when progressively removing observations for fitting. This high stability (insensitivity to the number of available observations for fitting) is due to the fact that in this case only two regression coefficients must be determined. Nevertheless, the low flexibility of linear regression models tends to prevent the interpolation of the data points and causes the linear regression model not to outperform cubic spline interpolation when many observations are available.

Fig.~\ref{fig:boxplot_models} in the Appendix summarizes the performance of the three models when using $[\numrange{4}{13}]$ number of observations.
Here it can be seen that the RMSE for the weir equation is higher on average respect to the cubic spline interpolation but still quite low (in the order of $\approx \SI{1}{\centi\meter}$).
Its variation is almost inexistent; the box plot is practically described by a line.\\

An in-depth comparison between the two best models (see Fig.~\ref{fig:fitting_std}) shows new interesting features.
The cubic spline interpolation is very sensitive to which observations are removed when few observations are available for interpolation. The standard deviation on its RMSE by changing which observations are removed increases by a factor of \num{60} when \num{4} resp. \num{13} observations are used.
Oppositely, the regression line based on the weir equation model is more sensitive to which observation is removed when only few of them are removed; while it becomes less sensitive as fewer observations are used to fit the model. However, the variation in RMSE variability  when \num{4} resp. \num{13} observations are used, changes only by a factor of $\approx \num{6}$.

When considering this, the regression line based on the weir equation can still have the potential to perform better than the cubic spline interpolation when only up to \num{6} observations are available (and not just by \num{4} as it was indicated by Fig.~\ref{fig:fitting_errors})

In the weir equation model proposed by \cite{brown_urban_2009}, the coefficient $C$ was allowed to vary as a function of $h_w$:  the shorter the weir width, the wider becomes the range of values for the $C$ coefficient. If the regression line would have been fitted on this model, then a better fit to the observations would have been obtained.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/fitting_std.png}
  \caption{Closer comparison of the weir equation and the cubic spline interpolation models.}
  \label{fig:fitting_std}
\end{figure}


This first case studies highlighted the importance of the trade-off between \emph{prior knowledge} and \emph{data availability}.
Machine learning (ML) regression algorithms such as \emph{Random Forest} \autocite{breiman_random_2001}, \emph{Gradient Boosting Machines} \autocite{friedman_greedy_2001} and \emph{Feedforward Neural Networks} \autocite{james_introduction_2013} are known to be very powerful at establishing non-linear functional relationship hidden in the data, but they also are particularly data-greedy. They have the potential to perform very well when a big training dataset is available, but fail if not enough data are available. Using numerical simulators, the generation of these large training datasets can be computationally very costly, thus partially hampering the use of these algorithms.

In this case study, it was shown that simpler interpolation methods can still allow to obtain good performance when  moderate simulated input-output sets are available. 
Moreover, the encoding of prior knowledge was proved to be useful when less data are available: the \emph{weir equation} model, unlike the others, could still perform well when fitted to few observations.

Taking into consideration the previous findings, GPs could be the ideal tool for emulation of physical processes due to their ability to model highly non-linear relationships, to interpolate the data points as well as encoding prior knowledge of the process \autocite{rasmussen_gaussian_2010}. Recent studies showed the possibility to direct map partial differential equations (PDEs) into covariance functions \autocite{lindgren_explicit_2011}. This allows to include the mechanistic of the process in the interpolation problem. On the other side, identification of accurate GP models can shed new light on the mechanistic of the process.

\newpage

\noindent{\LARGE\textbf{Case study 2}}
%===============================================================================
\section{A prelude to an early flood warning system}
\label{sec:hydrological_emulator}
%===============================================================================

As already stated in the introduction, our society and its infrastructures are exposed to the risk of flooding.
In this case study, we illustrate a methodology which can be used to develop an emulation-based early flood warning system.
For a channel conveying water from a catchment, we want to estimate, at a specific location along the channel, the time needed to reach a specific \emph{threshold discharge} ($Q_!$). 
The time needed to exceed $Q_!$ will be referred hereinafter as \emph{time-to-threshold} ($t_!$). 
This time-to-threshold will depend on the rainfall intensity on the catchment and the initial soil moisture content.

With the term "rain event", we are going to consider a combination of the \emph{rain intensity} and the \emph{initial soil saturation} over the catchment. 
For some rain events, namely very low precipitation and low initial soil saturation conditions, the threshold discharge at the specific location may never be reached.
For this reason, the developed warning system has a hierarchical structure: it is composed of a classifier determining whether the threshold discharge ($Q_!$) will be exceeded and of an emulator which estimates the time-to-threshold $t_!$.

To develop the methodology, a synthetic catchment topography was generated. Due to the generalization and abstraction of the proposed work-flow however, the methodology can be easily applied to real-situations. 

%-------------------------------------------------------------------------------
\subsection{Material and methods}
%-------------------------------------------------------------------------------
%...............................................................................
\subsubsection{Generating the topography}
%...............................................................................

The synthetic topography used is displayed in Fig.~\ref{fig:topography} and covers a catchment of size $\SI{2}{\kilo\meter} \times \SI{2}{\kilo\meter}$. 
The simulation grid is composed of $\num{100} \times \num{100}$ cells, which results in a cell resolution of $\SI{20}{\meter} \times \SI{20}{\meter}.$

The topography is generated by the superposition of a sloping plane with three Gaussian bumps.
The Gaussian bumps have different heights and widths and generate a \emph{Y-shaped channel} which extends from the upper and left boundary down to the lower boundary.
This has a single outlet located close to the center of the domain's bottom boundary.

In contrast to a real topography, the synthetic topography has a much smoother surface. This allows the use of a coarse grid resolution without losing the topographical features and the reduction of the simulation runtime. 
In a real situation, a wigglier topography might introduce additional non-linearities in the runoff-generation process, which would require an higher resolution of the simulation grid and thus longer computing time.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/topography.png}
  \caption{Synthetic topography composed of three Gaussian bumps on a sloping plane. The runoff from this domain, due to the high variability of its slope, is given by the superpositions of different waves. For the time-to-threshold flood-warning problem, this generates non-monotonous functions which make emulation challenging (see Fig.~\ref{fig:hydrograph}).}
  \label{fig:topography}
\end{figure}

%...............................................................................
\subsubsection{Setting-up the simulations}
%...............................................................................
The dataset required for building the emulator was generated by running \num{50} simulations with different combinations of the variables \emph{rain intensity} ($I$) and \emph{initial soil saturation} ($\theta_i$).
The duration of every simulation was $\approx \SI{30}{\minute}$, for a simulated time of \SI{9}{\hour}, giving a ratio real-time/simulation-time of \SI{3.3}{\minute\per\hour} simulated.
The initial soil saturation, despite being a spatially distributed variable, was kept uniform over the whole domain.
The rain intensity was kept constant and was uniformly applied over the domain, also because \citetalias{delestre_fullswof:_2017}, at this stage of development, only allows for uniformly distributed rain \autocite{laguerre_documentation_2016}.

\num{5} different initial saturations in the $[\numrange{0}{1}]$ interval and \num{10} rain intensities in the \SIrange{10}{35}{\milli\metre\per\hour} interval were taken and all their possible combinations were used as inputs for the simulations.
The rain duration was set to \SI{6}{\hour}, while a simulation duration of \SI{9}{\hour} was chosen in order to be able to observe the hydrograph recession. A time resolution of \SI{60}{\second} was used.

Tab.~\ref{tab:simulations_parameters} reports other simulation parameters that were kept constant over all the simulations.
The parameters marked with * are those \emph{spatially distributed}, meaning that a different value could be set for every cell.
For simplicity, a uniform catchment was used, therefore the values from the table are valid for the whole domain.
For the bottom boundary a \textit{Neumann boundary condition} was selected, allowing water to freely outflow.
Three \textit{wall boundary conditions} were set for the remaining boundaries, making sure that all of the water is lost through the bottom one.

\begin{table}[h]
  \centering
  \caption{Parameters and setting fixed for all simulations. Parameters marked with * are spatially distributed. For simplicity, these were kept uniform for the whole domain.}
  \label{tab:simulations_parameters}
  \begin{threeparttable}
    \begin{tabular}{lrl}
      \toprule
      \textbf{Parameter} & \textbf{Value} & \textbf{Units} \\
      \midrule
      Domain x-length                          &    $\num{2000}$           & \si{\meter}   \\
      Domain y-length                          &    $\num{2000}$           & \si{\meter}   \\
      Number of cells x                        &    $100$             &    \\
      Number of cells y                        &    $100$             &    \\
      Friction coefficient\tnote{*}            &    $0.03$            & \si{s.m^{-1/3}}\\
      Crust thickness\tnote{*}                 &    $1$               & \si{\meter}\\
      Crust hydraulic conductivity\tnote{*}    &    $2\cdot 10^{-6}$  & \si{\meter\per\second}\\
      Soil hydraulic conductivity\tnote{*}     &    $2\cdot 10^{-6}$  & \si{\meter\per\second}\\
      Soil suction head\tnote{*}               &    $0.09$      & \si{\meter}\\
      Soil maximum infiltration rate\tnote{*}  &    $19.8$      & \si{\milli\meter\per\hour}\\
      \bottomrule
    \end{tabular}
  \end{threeparttable}
\end{table}

%...............................................................................
\subsubsection{Extracting the datasets}
%...............................................................................
The datasets used to build the emulator was extracted from the simulation outputs in two steps.
First, the outflow hydrographs were extracted from the simulation output files, by summing up, at every time-step saved, the cell discharge along the whole domain bottom boundary. The result of this operation is a discharge time series with \num{540} observations, with a temporal resolution of 1 minute.
The same procedure was repeated for each of the 50 events, producing \num{50} different hydrographs (see Fig.~\ref{fig:hydrographs3d}).\\

Successively, the time-to-threshold $t_!$ was computed based on the specified threshold discharge $Q_!$. 
Simulated $t_!$, paired with the corresponding rain intensity and the initial soil saturation condition, formed the output-inputs set of observations generating the \emph{training dataset} used to construct the emulator. 

In a real situations, $Q_!$ could be the discharge at which a bridge or the embankments located on the channel are overflown. In order to develop the methodology, $Q_!$ was set to \SI{10}{\percent} of the $Q_{max}$ value recorded, corresponding to \SI{0.17}{\cubic\meter\per\second}.
 
By looking at one of the hydrographs extracted (Fig.~\ref{fig:hydrograph}) it can be observed that $t_!$ is a difficult quantity to predict. In fact, depending on the $Q_!$ set, the $t_!$ can be reached very quickly, can show a long time-lag, or not be reached at all. 
As illustratory example, in Fig.~\ref{fig:hydrograph}, the hydrograph between $Q_!$ and the $Q'_!$ presents a plateau. Thus, a minimal variation of the threshold discharge specification can lead to sudden large jump of the time-to-threshold. For this reason, the studied quantity $t_!$ is said to be \emph{discontinuous}.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/hydrograph.png}
  \caption{Response hydrograph for $\theta_i = \num{0.5}$ and $I = \SI{32.2}{\milli\meter\per\hour}$. By looking at the plateau located between the $Q_!$ and $Q'_!$ lines, the discontinuity in $t_!$ can be observed. A minimal change in the threshold discharge causes a jump of the time-to-threshold of almost an hour. This feature makes the emulation task very challenging.}
  \label{fig:hydrograph}
\end{figure}

%...............................................................................
\subsubsection{Building-up the classifier}
%...............................................................................

The classifier was built in order to ease the task of computing the time-to-threshold.
The classifier should distinguish whether a rain event is going to reach the threshold discharge or not.
Only the rain events classified as "reaching the discharge threshold" are given to the emulator to compute \emph{when} this will happen.

To train the classifier we assigned the value \num{-1} to rain events not reaching the discharge threshold and \num{1} the others.  
We tested different GP classifier before finding a satisfying one.
Since the data points seemed to be linearly separable, we decided to use the "meanLinear" mean function and vary
the covariance functions.
To visually inspect the performance of each of these different classifiers, we drew the classifier boundaries over the input space formed by the rain intensities and the initial soil saturation conditions. Fig.~\ref{fig:classifier_rough} shows the classifier boundary obtained by using a "covConst" covariance function.

However, an infinite number of curves could define the frontier between rain events exceeding and not the threshold discharge. Thus we decided to perform new simulations using combinations of rainfall intensity and initial soil saturation condition located in proximity of the classifier boundary. 

The data points obtained by these new simulations (marked as triangle in \ref{fig:classifier}) suggested that the boundary was actually not linear but curvy. 
We therefore trained various GP classifiers by testing various curvy mean function and a set of covariance functions.


%...............................................................................
\subsubsection{Building-up the emulator}
%...............................................................................
The procedure we used to build the time-to-threshold emulator is similar to the one used to build up the classifier.
To select an accurate GP model and evaluate its performance, we performed other simulations with \citetalias{delestre_fullswof:_2017} to also create a : 

\begin{enumerate}
   \item \emph{validation dataset} : composed of \num{36} different $(I, \theta_i)$ combinations covering regularly the regions of the input space not covered by the training data. These data points are displayed in blue in Fig.~\ref{fig:emulator} , while the code found to generate this validation observations is provided in Sec.~\ref{sec:test_dataset} of the Appendix.
   \item \emph{test dataset} : composed of \num{6} $(I, \theta_i)$ observations located sparsely in the inputs space. These observations are displayed in green in Fig.~\ref{fig:emulator}, and the $(I, \theta_i)$ pairs values  are reported in Tab.~\ref{tab:validation_dataset} of the Appendix.
\end{enumerate}

To define an accurate GP model, we tested various combinations of mean and covariance functions. 
The GP models trained on the training dataset (composed of 50 observations) were evaluated on the validation dataset. The mean and covariance functions of the GP model showing the lowest RMSE (computed on the validation set) was selected to build the final emulator. 

In order to increase the details captured by the final emulator, the training and validation datasets were combined together, and the resulting set of observations used to train the final GP model. 
 
The performance of the final emulator was assessed on the test dataset, by computing the RMSE and the maximum absolute error (MAE).

%-------------------------------------------------------------------------------
\subsection{Results}
%-------------------------------------------------------------------------------
%...............................................................................
\subsubsection{Simulations}\label{sec:simulations_results}
%...............................................................................

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{Figures/hydrographs3d.png}
  \caption{Response hydrographs for the \num{50} simulations at the catchment outlet which compose the training dataset. The red frame shows the end of the rain event. Depth-dimension shows the \emph{rain intensity} variable, while the \emph{initial saturation} variable is rendered by the color map. It can be observed, that shape and magnitude of the response vary considerably from one simulation to the other.}
  \label{fig:hydrographs3d}
\end{figure}


Fig.~\ref{fig:hydrographs3d} shows the \num{50} simulated hydrographs which compose the training dataset.
Many interesting features can be observed.
First of all, it can be seen that experiments run with the two lowest rain intensities and low initial soil saturation generated no discharge.
In the second place, simulations run with $\theta_i = \num{1}$ always reached their peak discharge, and this happened quite quickly.
Some of these show a second smaller increase after the first quick rising limb.
This is even more accentuated in the simulations were the soil was initially not saturated.
We can explain this effect as the mapping of the topography configuration to the response hydrographs.
The topography used presents zones with very steep slopes and others with very flat ones, causing different flow velocities.
When the water from the inclines has reached the outlet, that coming from flat areas is still traveling downstream.
After a given lapse of time this water reaches the outlet as well, giving rise to the bumps visible in the hydrographs.\\

%...............................................................................
\subsubsection{The classifier}\label{sec:classifier}
%...............................................................................

Fig.~\ref{fig:classifier} displays the results of the classification task.
Rain events reaching the discharge threshold are marked with red circles, while rain events not reaching it with green ones. Circles represent training dataset observations, whereas triangles indicate the dataset which was added in a second time to test the linearity of the boundary.
According to the classifier, every $(I, \theta_i)$ pair chosen in the yellow region will cause the exceeding of $Q_!$. The classifier could correctly classify all but one data points given.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/classifier.png}
  \caption{Classifier: the circles represent the first training dataset, while the triangles the second one. In red all events exceeding the threshold, in green all those which did not. Events in the yellow region are the ones classified as "exceeding the threshold", for which the time-to-threshold has to be estimated.}
  \label{fig:classifier}
\end{figure}


%...............................................................................
\subsubsection{The emulator}
%...............................................................................
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/emulator.png}
  \caption{Time-to-threshold emulator: training (red), validation (blue) and test (green) datasets and the emulator (mesh) predicting the time-to-threshold ($t_!$) in regions where there are no observations. The emulator interpolates all of the training observations (red and blue ones). The extrapolation far outside the observations boundaries are meaningless. These are already discarded by the classifier.}
  \label{fig:emulator}
\end{figure}

The time-to-threshold emulator is shown in Fig.~\ref{fig:emulator}.
The x and y axis indicate the value of the inputs (rain intensity and soil saturations) while the corresponding time-to-threshold is given on the z-axis.
The circles represent the datasets used for training (red), validation (blue) and testing (green).
The interpolation surface of the final emulator is given by the black mesh and is the result of the evaluation of the final GP model at \num{6400} mesh points.

The performance of the emulator was assessed by computing MAE and RMSE on the test dataset.
These provides an estimation of the out-of-sample error, and is reported in Tab.~\ref{tab:emulator_performance}.

\begin{table}[h]
  \centering
  \caption{Emulator performance on the \emph{test} datasets.}
  \label{tab:emulator_performance}
  \begin{tabular}{lcc}
    \toprule
     & \textbf{MAE [\si{\minute}, \si{\percent}]} & \textbf{RMSE [\si{\minute}]} \\
    \midrule
    \textbf{test} & $6.0,\,5.9$ & $4.0$\\
    \bottomrule
  \end{tabular}
\end{table}

The emulator's speedup factor was also computed.
The evaluation of the emulator at one point lasts in average \SI{0.012}{\s}, whereas running a simulation takes approximately \SI{30}{\minute}.
This represents a \emph{speedup} of \textbf{\num{150000}} $\bm{\times}$.

%-------------------------------------------------------------------------------
\subsection{Discussion}
%-------------------------------------------------------------------------------

In early flood warning system, a crucial point is not to underestimate the danger of floods.
In case a flood alert is issued but no flood occurs, minor economic loss could be related to deployment of temporary flood control measures or the cost of population evacuations, but nobody would be armed. 
In the opposite scenario, when no alert is issued but a flood occurs, the risk of elevated economic losses and  fatalities occurrence is high. 
 
Our emulation-based early flood warning system is composed of a \emph{classifier} which says if an alert should be emitted, and by an \emph{emulator} which estimates the time period before a specific discharge is exceeded in case an alert is emitted by the classifier.

Dramatical consequences could thus happen if the classifier would not emit an alert, but the specified discharge is exceeded.\\

In Sec.~\ref{sec:classifier} we saw that the proposed classifier failed to classify one observation.
In that case, the classifier predicted that the discharge threshold would have been exceeded when actually was not. However the misclassified point is a training point, which means that the established classifier is not so accurate and a better one should be identified. 
To do this, new simulations near the actual classifier boundary should be performed , new GP models tested and evaluated. This iterative procedure has not be done due to lack of time, but should be conducted if such type of emulator would be applied in a real situation.  

As far as the estimation of time-to-threshold is concerned, underestimation of the period of time before the threshold discharge is reached has less consequences than an overestimation of it. This because the flood mitigation measures (for the expected flood) would already be in place. 
Tab.~\ref{tab:validation_performance} in the Appendix reports the errors in time-to-threshold for each observation of the test set.

Regarding underestimation of $t_!$, the worst error in term of magnitude on the test set is when estimating the flood to occur after \SI{198.0}{\minute} when actually occurring \SI{5.5}{\minute} before, based on the numerical simulations of test observation 2.
This means a relative time-to-threshold error of about 3 \% to the simulated time-to-threshold. 

However, when having to put in place flood mitigation measures, high accuracy in determining the time-to-threshold is required when the time period is short. In the case of the test observations 4 and 5, the relative time-to-threshold error is of about 4.5 \%. 

Although having obtained a time-to-threshold error of \SI{30}{\s}  when $t_!$ is in the order of \SI{10}{\minute} is an excellent result, more test observations should be simulated to obtain a reliable emulator performance assessment.
 
As a final remark regarding the emulator performance, it should be kept in mind that the time resolution of the numerical simulation was \SI{1}{\minute}. Thus, the test time-to-threshold error with magnitude smaller than \SI{1}{min} should not be considered de facto as errors. This is the case for test observations 4 and 6. 

In light of real-world applications, the results obtained with our emulator are over-optimistic.
The actual emulator performance does not account for the uncertainty in the inputs parameters. 
In real situations, rain intensity and initial soil saturation must be estimated or are known only to a given degree of accuracy. 
Therefore, propagation of inputs uncertainty to emulator outputs should be accounted when assessing the performance of an operational emulator.

Still, the advantage in using emulators is that uncertainty quantification can be performed much faster than using numerical simulators: the speed up obtained with our emulator was shown to be \num{150000} $\times$. 

In conclusion, the shorter computing time required by the emulator would also allow to adapt the predictions in near-real time: the emulator could be re-evaluated during the duration of the rain event if more accurate inputs conditions are available or integrating for example in the workflow a Kalman filtering data assimilation scheme. 

