\chapter{Case studies}
\label{chp:case_studies}
%-------------------------------------------------------------------------------

\noindent{\LARGE\textbf{Case study 1}}
%===============================================================================
\section{A mechanistic emulator: fitting the \emph{weir equation}}
%===============================================================================

%:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
%  * present it as a didactic example
%  * use it to compare GP (prior knowledge) with e.g. deep neural networks: how many points can we have?
%  * mention grid convergence study (results go in the Appendix)
%  * mention problem with FullSWOF boundary conditions
%  * define well results and methodology
%  * such an emulator can be improved -> modified to compute slope in order to have a given mu value
%  * state all of the goals of the CS: learn weir eq. from simulation, apply curve fitting, get a feeling for simulation accuracy
%:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

As a first case study to apply the acquired knowledge, it was decided to build an utterly mechanistic emulator.
The main goal of this case study is to try to fit the \emph{weir equation} to simulated data instead of to experimental data.
Here we basically rediscover the way science has always been done: by observing, measuring and trying to find mathematical relationships.
The biggest difference is the fact that the experimental set-up is completely computer built.\\

The \emph{weir equation} is a partially theoretical equation that provides an estimate of the discharge $Q$ over a weir as a function of the water depth above the weir itself ($h_w$).
The equation can be derived from the Bernoulli equation under certain assumptions \autocite{bos_discharge_1989} and can be found under many different forms.
The form used here is the one proposed by \emph{Francis} \autocite{walcott_weir_1907}:

\begin{equation}\label{eq:weir_eq}
  Q = \textcolor{red}{C} \cdot L \cdot h_w^{\textcolor{red}{a}}\, , \quad \mbox{usu. } \textcolor{red}{a = 3/2}
\end{equation}

\noindent Where the empirical coefficient $C$ corrects for the assumption of absence of viscous effects and uniform velocity distribution. $L$ is the length of the weir (perpendicular to the channel) and $h_w$ is the water height above the weir.

In addition to the parametric model (weir equation) two non-parametric local techniques, namely \emph{linear interpolation} and \emph{cubic spline interpolation}, were used to intrapolate between the simulated data points.\\


%-------------------------------------------------------------------------------
\subsection{Brief experiment description}\label{sec:cs1_brief_description}
%-------------------------------------------------------------------------------

\seb{explain the experiment very generally. Put the section title or leave it with no section title with the upper part?}
 
For this experiment simulations were run in a flat rectangular channel.
A weir with a trapezoidal cross section is located at the channel half-length.
As initial condition the upstream side of the weir was filled with water up to the weir crest.
At the domain top boundary a constant inflow discharge was set, while at the bottom boundary water could freely outflow.
As the simulation runs, the inflow water flows down the channel, overflows the weir and leaves the domain through the lower boundary.
After some time the simulation reaches the \emph{steady-state} conditions: inflow, discharge over the weir and outflow have the same magnitude and the water height above the weir has stabilized.
At this point the value $h_w$ was extracted and was coupled with the discharge value $Q$ generating it.
\num{25} experiments were conducted with $Q$ linearly spaced in the range \SIrange{0.1}{10}{\cubic\meter\per\second}.
All simulated pairs $(Q, h_w)$ constitute an observational \emph{predictor-response} set to which the weir equation was fitted.
 \seb{give e.g. in Appendix the discharge values used? Probably not, I gave the code with linspace to generate the different discharges}\\

In order to ensure the convergence of the simulator solution, and therefore the quality of the experimental results, a \emph{grid convergence study} was performed prior to the experiment.
For this, the simulation with the highest discharge was repeated with successive grid refinements
The value of the variable of interest, water height, was then compared between the different simulations to find at which grid resolution the solution stabilizes.


%-------------------------------------------------------------------------------
\subsection{Material and methods}
%-------------------------------------------------------------------------------
%...............................................................................
\subsubsection{Generating the topography}
%...............................................................................
The topography used for running the simulations was generated in \textit{Octave} \seb{write "Octave", "FullSWOF", ... in italics?} and represents a flat channel of \SI{40}{\meter} length with a weir placed at its midpoint, at \SI{20}{\meter} distance from the top boundary.
The channel cross section is a rectangle of \SI{4}{\meter} width and the weir has a trapezoidal shape.
Fig.~\ref{fig:weir_scheme} shows the geometry of the channel.
The weir has a crest width of \SI{2}{\meter} and therefore belongs to the \emph{broad-crested} class. The $C$ coefficient for broad-crested weirs with vertical walls and \SI{2}{\meter} crest width varies between \num{1.36} and \num{1.53}, depending on the water height $h_w$ \autocite{brown_urban_2009}.
We therefore expect a value close to this for our experiment.
A 3D overview of the channel can be found in Fig.~\ref{fig:channel} in the Appendix. \seb{add a 3D plot of the channel?}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/weir_scheme.png}
  \caption{Geometry of the weir used for the experiments.}
  \label{fig:weir_scheme}
\end{figure}

%...............................................................................
\subsubsection{Setting-up the simulations}
%...............................................................................
After generating the topography to be used for the experiment the simulation parameters were defined and saved to the parameters file.

The simulation was done with no rain and no infiltration.
This way, water just enters the system through the top boundary and leaves it through the bottom one.
For this a \emph{Neumann} boundary condition was chosen at the bottom, while an \emph{imposed discharge} boundary condition was set at the inflow boundary.
This boundary condition needs two values to be specified: the magnitude of the inflow and an imposed water height, which is used under supercritical flow conditions.
The \num{25} inflow discharges used were obtained with the following \citetalias{octave_community_gnu_2018} code.\\

\inputminted[
  fontsize=\footnotesize,
  firstline=4,
  lastline=5,
  numbersep=2pt,
  gobble=0,
  frame=none,
  bgcolor=light-gray,
  framesep=10mm
]{octave}{code.m}\\

\noindent Where the minus sign indicates that water steadily \emph{enters the domain}.
The imposed height was set to \SI{3}{\m}, corresponding to the weir height.

The channel topography presents no banks.
Wall boundary conditions were set for the lateral boundaries in order to produce the rectangular cross section.
The wall boundaries extend infinitely high, preventing the water from overflowing.
Simulations were run for $t_{max} = \SI{200}{\s}$ and \num{200} intermediate states were saved, giving a time resolution of the simulation output of \SI{1}{\s}.
For the channel a uniform Manning roughness coefficient of \SI{0.03}{\s\per\m\tothe{1/3}} was used.

Since infiltration was not active for the simulations, all of the parameters relative to the soil model did not have to be set.

The number of cells in x, and in y direction were defined according to the grid convergence study explained here below.

%...............................................................................
\subsubsection{Conducting the grid convergence study}
%...............................................................................
\seb{better to keep a separated section, including methods and results like done here, or split it up under "material and methods" and "results and discussion"?}

Accuracy of simulation results, as well as simulation runtime, are dependent on the grid resolution chosen. \seb{turn the sentence around in order to highlight the point of the paragraph: grid resolution.}
In order to find an appropriate grid resolution, a grid convergence study was performed.
Successive refinements were tested in order to find out from which grid resolution the solution stabilizes, becoming independent from the resolution used.
A criterion has to be established, in order to decide when the solution has converged to a satisfactory result.
If the Grid Convergence Index (GCI) is used, then a GCI lower than \SI{5}{\percent} for the last refinement is usually taken as a criterion \autocite{ali_grid_2009}.

For the mesh study \num{7} simulations were run by varying the grid resolution ($Nx$, $Ny$) only. \seb{those are mentioned in the "list of symbols". Does this help understand what was varied?}
Topography and parameters used are those mentioned in the two preceding sections.
Squared cells were used and the inflow discharge was set to the highest discharge value of the experiment, namely \SI{10}{\cubic\meter\per\second}.
This discharge is the one generating the highest flow velocities.
If convergence under these conditions is reached, then convergence for lower discharges is also assured.
Tab.~\ref{tab:mesh_study} summarizes the simulations main characteristics.

\seb{how to indicate units in the table?}
\begin{table}[h]
  \centering
  \caption{Summary of simulations runtime, grid resolution and other related parameters for the mesh convergence study.}
  \label{tab:mesh_study}
  \begin{tabular}{crrcccccr}
    \toprule
%    \rowfont{\bfseries}
    \multirow{2}{*}{\#} & \multicolumn{1}{c}{Nx} & \multicolumn{1}{c}{Ny} & \multicolumn{1}{c}{Lx} & \multicolumn{1}{c}{Ly} & \multicolumn{1}{c}{dx} & \multicolumn{1}{c}{dy} & \multicolumn{1}{c}{Q} & \multicolumn{1}{c}{runtime} \\
       & \multicolumn{1}{c}{$/1$} & \multicolumn{1}{c}{$/1$} & \multicolumn{1}{c}{$/\si{\meter}$} & \multicolumn{1}{c}{$/\si{\m}$} & \multicolumn{1}{c}{$/\si{\m}$} & \multicolumn{1}{c}{$/\si{\m}$} & \multicolumn{1}{c}{$/\si{\cubic\m\per\s}$} & \multicolumn{1}{c}{$/\si{\minute}$} \\ 
    \midrule
    1  & 2             & 20            & 4               & 40          & 2.00        & 2.00        & 10                      & 0.00 \\
    2  & 4             & 40            & 4               & 40          & 1.00        & 1.00        & 10                      & 0.07 \\
    3  & 8             & 80            & 4               & 40          & 0.50        & 0.50        & 10                      & 0.60 \\
    4  & 20            & 200           & 4               & 40          & 0.20        & 0.20        & 10                      & 9.00 \\
    5  & 40            & 400           & 4               & 40          & 0.10        & 0.10        & 10                      & 69.60 \\
    6  & 80            & 800           & 4               & 40          & 0.05        & 0.05        & 10                      & 537.72 \\
    7  & 100           & 1000         & 4               & 40          & 0.04        & 0.04        & 10                      & 1048.40 \\
    \bottomrule
  \end{tabular}
\end{table} \seb{put runtime here? Or somehow separate it, or do not put it at all?}

Fig.~\ref{fig:water_profiles} shows the free surface profiles of the \num{7} simulations at steady-state conditions along the channel axis.
It can be noticed that for $Ny =$ \numlist{20;40;80} the water depth is visibly higher than for further refinements, and its shape quite varying.
This means that the solution has not converged yet.
Results for $Ny =$ \numlist{400;800;1000} are very close, since the lines are almost overlapping.
$Ny =$ \num{200} shows a similar profile shape, but the height is still diverging quite a bit from the successive refinements.
To better observe the variations between the different refinements, the $h$ value convergence at the weir crest was studied.

\seb{how to indicate units in graphs / diagrams? (unit)? [unit]? /unit?}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/water_profiles.png}
  \caption{Longitudinal free surface profiles at steady-state conditions for the \num{7} different grid resolutions.}
  \label{fig:water_profiles}
\end{figure}

In Fig.~\ref{fig:diff_center} the percent variation in the value of $h$ can be observed.
Measured absolute $h$ values at the weir center can be observed in Fig.~\ref{fig:convergence_center} in the Appendix. 
At the \nth{4} refinement the percent variation is smaller than \SI{0.01}{\percent}.
This variation is small enough to assert that the solution has satisfactory converged.
For the experiment it was therefore decided to use $(Nx, Ny) = (\num{40}, \num{400})$ corresponding to a grid resolution of \SI{0.10}{\m} in both x and y directions. The simulation runtime of $\approx \SI{1}{\hour} \SI{10}{\minute}$, required for this resolution is still quite acceptable.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/diff_center.png}
  \caption{Percent variation of the measured $h$ at the weir midpoint between the given $dy$ and the previous one.}
  \label{fig:diff_center}
\end{figure}
\seb{Figure 3.3 moves to another section... is that ok??}

%...............................................................................
\subsubsection{Extracting the dataset}
%...............................................................................
The weir equation computes the discharge over the weir as a function of the water height $h_w$ under steady-state conditions.
These are reached after $\approx \SI{50}{\s}$ of simulation.
After this time, small oscillation of the water surface are still present.
To get rid of these, a time averaged free surface from $t = \SI{100}{\s}$ to $t = \SI{200}{\s}$ was computed.
Fig.~\ref{fig:free_surfaces} presents the free surface profiles of the \num{25} experiments at steady-state conditions obtained after this \emph{time smoothing out}.
The three lowest profiles correspond to the simulation runs with the three lowest inflow discharges. 
As initial condition the weir upstream side of the channel was filled with water to the weir's height.
The fact that these three profiles are lower than the initial condition indicates that something went wrong with the simulation and water was lost from the top boundary.
These simulations were discarded for the continuation of the experiment.
The height value for the remaining simulations was extracted \SI{1.2}{\m} before the weir base, to avoid observations in the acceleration zone, happening in proximity of the weir crest.
At this point a space average over the channel breadth was taken.
The procedure was repeated for all of the experiments.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/free_surfaces.png}
  \caption{Free surface profiles along the channel axis for the \num{25} experiments.}
  \label{fig:free_surfaces}
\end{figure}


%...............................................................................
\subsubsection{Fitting the data}
%...............................................................................
The weir equation (Eq.~\ref{eq:weir_eq}) was fitted to the dataset previously extracted using linear regression.
The original weir equation is non-linear with respect to the parameters, it was thus linearized by taking the logarithm:

\begin{equation}
  \log(Q) = \log(C) + \log(L) + a \cdot \log(h_w)
\end{equation}

\noindent and the values of the parameters $a$ and $C$ were determined.
The performance of the fitted weir equation should be compared with that of a linear interpolation through the points and that of a cubic spline interpolation.
Therefore, the points were interpolated with both methods.


%...............................................................................
\subsubsection{Computing the error}\label{sec:compute_error}
%...............................................................................
To evaluate the performance of the three models, an increasing number of observations were removed, the models were trained on the remaining observations and it was tried to predict the ones removed.
For an amount $k$ of points removed all possible combinations were tested. The number of model evaluations follows:

\begin{equation}
  \binom{n}{k} = \frac{n!}{k!\left(n-k\right)!}
\end{equation}

\noindent Since this number grows very fast a subset of the initial dataset was used.
This can be found in Tab.~\ref{tab:dataset_error} in the Appendix.
The first and last points of the dataset were kept during all the \emph{cross-validation} experiment in order to avoid wild extrapolation at the boundaries of the predictor space.
From the remaining \num{12} points all possible combinations of \num{1} to \num{10} points were removed.
The root mean squared error (RMSE) of every combination was computed and the average RMSE for the removal of \num{1} up to \num{10} points was taken.


%-------------------------------------------------------------------------------
\subsection{Results}
%-------------------------------------------------------------------------------
% * importance of prior knowledge -> even if few points we can obtain good
%   models. Not the case without (lin. interp, spline interp.)


In Fig.~\ref{fig:simulations_results} the $(Q, h_w)$ pairs extracted from the simulations are plotted.
The gap in the lower part of the plot is due to the data that had to be discarded.
These points represent the dataset which was used to fit the weir equation and the other two models.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/simulations_results.png}
  \caption{Plot of the $(Q, h_w)$ pairs extracted from the simulations. Points corresponding to the unsuccessful simulations have already been discarded. This produces the gap visible in the left part of the plot.}
  \label{fig:simulations_results}
\end{figure}

The fit of the different models to the dataset is displayed in Fig.~\ref{fig:fitting_results}.
It can be observed that the linear interpolation fails to capture the initial trend shown by the weir equation and the cubic spline interpolation.
This is due to two main reasons.
The first one is that the density of data points in this region is low---the linear interpolation simply joins the two nearest points with a segment---and on such a long distance it can diverge quite a lot from non-linear models.
Moreover, in this region the weir equation presents its maximum curvature.
This makes the linear interpolation diverge even faster.

From the second to the last point all models seem to perform good: the three lines are almost perfectly overlaying, meaning that all give very similar results.
Some divergences can be observed in proximity of data points, especially between the weir equation and the other two models: this is due to the fact that the two interpolations \emph{interpolate} the points, whereas the curve obtained for the weir equation represents the best linear fit to the points (\mintinline{octave}{polyfit (x, y, 1)}).
This minimizes the least-squares-error of the fit \autocite{eaton_gnu_2016}, which does not necessarily means interpolating through the points.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/fitting_results.png}
  \caption{Fit of the three models to the simulated dataset. In blue the weir equation with $C = \num{1.65}$ and $a = \num{1.59}$, in red the linear interpolation through the points and in green the cubic spline interpolation.}
  \label{fig:fitting_results}
\end{figure}

For the fitting of the weir equation the values $C = 1.65$ and $a = 1.59$ were found.
As mentioned in Sec.~\ref{sec:cs1_brief_description} for a weir of \SI{2}{\m} breadth and vertical walls $C$-values in the range $[\numrange{1.36}{1.53}]$ are expected, depending on the magnitude of $h_w$.
The value obtained for $C$ assumes that $C$ does not vary with $h_w$.
Moreover, the weir used for the experiment has a trapezoidal cross section instead of a rectangular one.
\cite{tracy_discharge_1957} investigated how the coefficient $C$ varies with different weir shapes.
For a weir with sloping walls the $C$ coefficient increases, because the head losses in comparison to one with vertical walls are lower.
The difference in the $C$ coefficient observed here can be explained by this phenomenon.\\

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/fitting_errors.png}
  \caption{Mean RMSE of the three different models as a function of the number of points used to train the models.}
  \label{fig:fitting_errors}
\end{figure}

In Fig.~\ref{fig:fitting_errors} the RMSE obtained by cross-validation as explained in Sec.~\ref{sec:compute_error} are displayed.
No matter how many points are used, the linear interpolation always performs at worst.
By increasing the number of points its performance improves quite rapidly, but not enough to reach that of the other two models.
The process generating the points does not have a linear behavior.
Despite the improvement of its performance, this model will never approximate the process generating the points as well as the others.

The results for the cubic spline interpolation confirm what was already observed in the previous plot.
This is a good model at mimicking the process originating the points.
Its performance goes from slightly worse than that of the weir equation model up to almost \num{2} order of magnitude better.
However, by reducing the number of points, the error of this model increases exponentially.
Thanks to its flexibility, the cubic spline interpolation shows very small errors when fitted with many points.
This model, by definition, interpolates through all of the training points.
If the training points are close to each other, the chance of interpolating an "unseen" point in the middle of two others, or at least to pass very close are very high.
By increasing this distance these chances decrease quite fast, since the cubic spline interpolation is not a very suitable model to describe the behavior of the process between two points.
For this reason the error increases exponentially.

The weir equation model used minimizes the least-square-error of the fit, without necessarily interpolating through the points.
It is a very inflexible model, only two parameters can be varied to improve the quality of fit, while many more observations are available.
For this reason the interpolation through the points cannot be achieved, unless the observations were originated exactly by the model chosen.
The results shown here are the best which can be achieved with it.
Because interpolation cannot be obtained, its error decreases very slowly with increasing number of points.
The inflexibility of this model represents its weakness but at the same time its strength.
This is in fact a very robust model, which still produces good results when few observations are available.
It shows almost no decline in performance by reducing the number of points.

Although its performance is outperformed by the cubic spline interpolation, when more than \num{4} observations are taken, use of this model can still be considered: the error, in spite of being higher than the one obtained with the cubic spline interpolation, is still very little compared with the values of $h$ measured.
In a practical application in hydraulics the difference between the two models could be neglected and one could opt for the most stable one, to stay on the safe side when few points are available.
Fig.~\ref{fig:boxplot_models} in the Appendix summarizes the performance of the three models when using $[\numrange{4}{13}]$ \# of points.
Here it can be seen that the RMSE for the weir equation is higher on average respect to the cubic spline interpolation but still very low (in the order of $\approx \SI{1}{\centi\meter}$).
Its variation is almost inexistent, the box plot is practially described by a line.\\

A closer comparison between the two best models (Fig.~\ref{fig:fitting_std}) shows new interesting features.
The cubic spline interpolation is very sensitive to which points are removed when many are removed.
The standard deviation on its RMSE by changing which observations are removed increases by a factor of \num{60} when \num{4} resp. \num{13} points are used.
The opposite is observed for the weir equation model.
This is more sensitive to which observation is removed when only few of them are removed and becomes less sensitive as fewer observations are used to fit the model.
However, the variation here when \num{4} resp. \num{13} points are used is only $\approx \num{6} \times$.

When considering this, the weir equation can still do better when up to \num{6} points are used, and not just by \num{4} as it was indicated by Fig.~\ref{fig:fitting_errors}.
In the weir equation model proposed by \cite{brown_urban_2009}, the coefficient $C$ varies as a function of $h_w$.
The shorter the weir width, the wider becomes the range of values for the $C$ coefficient.
If a functional relationship $f:h_w \mapsto C$ had been established, then a weir equation model better fitting the data would have been obtained.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/fitting_std.png}
  \caption{Closer comparison of the weir equation and the cubic spline interpolation models.}
  \label{fig:fitting_std}
\end{figure}


From this example the importance of the trade-off \emph{prior knowledge} -- \emph{data availability} can be seen.
Some "interpolation methods" \seb{is it correct "interpolation methods"? how could I call them otherwise?}, e.g. \emph{neural networks} in machine learning (ML), are very data-greedy.
These can perform very well when a big training dataset is available, but fail if not enough data is available.
When obtaining new observations is computationally very costly, like it is the case with most simulations, "interpolation methods" allowing the encoding of prior knowledge can turn out to be very useful.
Gaussian processes (GP) represent a good example.
When the physical laws governing a process are known, i.e. its partial differential equations \seb{linear partial differential eq (to put them in the covariance function, there is a mapping)} the non-linear ones can be added in the mean function, it can be tried to encode them in the GP used.
If the information embedded is correct, the result will be very similar to the one observed with the weir equation case study: it does not matter how many points are available to perform the fitting, the results will still be good.
If the right model for the process is found, no more observation are needed.
These are simply given by the evaluation of the model itself.

Moreover the technique just explained can give further insight and can help us learn new functional relationships about the process which is studied.
If the GP chosen performs good, then the functional relationships encoded in it are somehow closely linked with the process itself.
New relationships and dependencies between predictors used and responses observed can be established and we can therefore shed new light on the mechanistic of the process.

\seb{should I add a plot of the residues, to see if the weir equation is the "actual model" generating the points?}

%-------------------------------------------------------------------------------
\subsection{Discussion}
%-------------------------------------------------------------------------------
%* finish with take home message



%-------------------------------------------------------------------------------
%An \textit{early flood warning tool} in an essential component of an \textit{early flood warning system}.
%An early flood warning system has to be understood as an integrated system of tools and plans to detect and respond to flood emergencies \autocite{icimod_early_2018}.
%This can be managed by the community themselves and if designed, implemented and operated correctly can make the difference between tragedy and survival.

%Such systems have already been installed in various endangered regions in the world.
%After the major flooding of July 2014, the city of Altstätten in the canton of St. Gallen made the decision to install one.
%The system installed uses cameras, sensors and level meters to gather data and information about the current situation \autocite{st._galler_tageblatt_altstatten_2017}.
%When the value of certain parameters exceed the given threshold, a dangerous situation is recognized and the alarm signal is sent.

%Three years after the installation of the system an alarm rings in the middle of the night.
%Firemen go immediately into action in order to install temporary measures to fight against the water.
%A couple of hours later the torrent overflows at several points and the city gets flooded.
%Damages are less severe than last time, especially thanks to the temporary measures installed, but possibly they could have been reduced even more.

%Crucial in order to limit the damages is the intervention time before the actual flooding occurs.
%The earlier the dangerous situation can be detected the more time is available to the population and authorities to get ready and set up different types of temporary mitigation measures.
%Systems based on sensors monitoring the evolution of the current situation in the upper part of the catchment are quite reliable but do not allow for long anticipation time.

%Numerical simulations can be run with meteorological forecast data and approximate soil saturation conditions in order to obtain early predictions of the event outcome.
%However, the big advantage of predicting with that much anticipation is partially lost due to the duration of such simulations.
%Accurate meteorological forecasts are available only few hours before the event.
%If the model require several hours to run, which is often the case to obtain accurate predictions for catchments of this extent, then the advantage of being able to run it in advance is canceled.

%A possible solution to this problem is the development of an \emph{early flood warning tool} based on an \emph{ad hoc surrogate model} exploiting the catchment specific behavior.
%This early flood warning tool should be able to recognize if a rain event will generate a channel discharge leading to flooding and if yes within how much time.
%For this scope two different emulators are used.
%The first emulator classifies a rain event based on the forecasted \emph{average rain intensity} and \emph{current soil saturation} into two groups: rain events generating discharge exceeding a chosen threshold ($Q_!$) and rain events not generating discharge exceeding the threshold.
%For events exceeding the threshold a second emulator is developed.
%This predicts the time the rain event will need to produce the threshold discharge $Q_!$ at the outlet of the catchment.
%-------------------------------------------------------------------------------


\newpage

\noindent{\LARGE\textbf{Case study 2}}
%===============================================================================
\section{An hydrological emulator: estimating the \textit{time-to-threshold}}
%===============================================================================

With this case study it was decided to address a complex hydrological problem by means of emulation.
The problem we want to solve is estimating the time needed for a channel conveying water from a catchment to reach a certain threshold discharge ($Q_!$) at a specific point along the channel when subject to different rains and when a different initial state of the soil is found.
With the term "rain event" we are going to consider both the actual rain and the initial conditions of the soil.
Two events characterized by the same rain but different initial conditions  of the soil represent two different rain events. The parameters chosen to characterize a rain event are \emph{rain intensity} and \emph{initial soil saturation}.

For the experiment a synthetic catchment topography was used, which at first glance can make the case study look very abstract.
In reality the emulator is just exploiting the catchment specific behavior, which is of course different from one catchment to another, but the methodology applied here could be applied to whatever catchment.
The emulator developed here, with the appropriate adjustments, could be exploited as an \emph{early flood warning tool}.

As for the previous example, the simulations were run with the open source software \citetalias{delestre_fullswof:_2017}, while the input files necessary for the simulations were prepared with \citetalias{octave_community_gnu_2018} with the aid of the package \textit{fswof2d} developed.


%-------------------------------------------------------------------------------
\subsection{Material and methods}
%-------------------------------------------------------------------------------
%...............................................................................
\subsubsection{Generating the topography}
%...............................................................................

The topography used represents a catchment of size $\SI{2}{\kilo\meter} \times \SI{2}{\kilo\meter}$ and it is visible in Fig.~\ref{fig:topography}.
It is composed of a sloping plane with three Gaussian bumps on the top.
The Gaussian bumps have different heights and widths and generate a \emph{Y-shaped channel} which extends from the upper and left boundary down to the lower boundary.
A paraboloid was added to the plane to promote the accumulation of water in the channel.
This has a single outlet located close to the center of the domain's bottom boundary.

The synthetic topography produced, in contrast to a real one, has the advantage of having a much smoother surface, which allows using a coarser grid resolution without losing topographical features.
The coarser the grid the fewer the nodes where the finite volume (FV) equations have to be solved, which reduces the simulation runtime \seb{how does it decrease? linearly? quadratically?}. 
Another advantage of having a smooth topography, not presenting discontinuities, is that the solutions converge easily, and no grid refinement is necessary.
The grid used is composed of $\num{100} \times \num{100}$ cells, giving a resolution of $\SI{20}{\meter} \times \SI{20}{\meter}.$

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/topography.png}
  \caption{Synthetic topography composed of three Gaussian bumps on a sloping plane.}
  \label{fig:topography}
\end{figure}

%...............................................................................
\subsubsection{Setting-up the simulations}
%...............................................................................
The dataset required for building the emulator was generated by running \num{50} simulations with different combinations of the variables \emph{rain intensity} ($I$) and \emph{initial soil saturation} ($\theta_i$).
The initial soil saturation, being a spatially distributed variable, was kept uniform over the whole domain.
The rain intensity was kept constant and was uniformly applied over the domain, also since \textit{FullSWOF\_2D}, at this stage of development, only allows for uniformly distributed rain events \autocite{laguerre_documentation_2016}.

\num{5} different initial saturations in the $[\numrange{0}{1}]$ interval and \num{10} rain intensities in the \SIrange{10}{35}{\milli\metre\per\hour} interval were taken and all their possible combinations were used as inputs for the simulations.
The rain duration was set to \SI{6}{\hour}, while a simulation duration of \SI{9}{\hour} was chosen in order to be able to observe the hydrograph recession. A time resolution of \SI{60}{\second} was used, which means that intermediate simulation results were written to the \emph{outputs} file every \SI{60}{\second}.

Some parameters, namely those specific for the catchment in questions, were kept constant over all of the simulations.
These are summarized in Tab.~\ref{tab:simulations_parameters}.
The parameters marked with * are those \emph{spatially distributed}, meaning that a different value could be set for every cell.
For simplicity, a spatially uniform catchment was used, therefore the values from the table are valid for the whole domain.
Three \textit{wall boundary conditions} were set, for the upper and the two lateral boundaries.
For the bottom boundary a \textit{Neumann boundary condition} was selected, allowing water to freely outflow through that boundary and making sure that all of the water is lost from here.

\begin{table}[h]
  \centering
  \caption{Parameters and setting fixed for all simulations.}
  \label{tab:simulations_parameters}
  \begin{threeparttable}
    \begin{tabular}{lrl}
      \toprule
      \textbf{Parameter} & \textbf{Value} & \textbf{Units} \\
      \midrule
      Domain x-length                          &    $2'000$           & \si{\meter}   \\
      Domain y-length                          &    $2'000$           & \si{\meter}   \\
      Number of cells x                        &    $100$             & --   \\
      Number of cells y                        &    $100$             & --   \\
      Friction coefficient\tnote{*}            &    $0.03$            & \si{s.m^{-1/3}}\\
      Crust thickness\tnote{*}                 &    $1$               & \si{\meter}\\
      Crust hydraulic conductivity\tnote{*}    &    $2\cdot 10^{-6}$  & \si{\meter\per\second}\\
      Soil hydraulic conductivity\tnote{*}     &    $2\cdot 10^{-6}$  & \si{\meter\per\second}\\
      Soil suction head\tnote{*}               &    $0.09$      & \si{\meter}\\
      Soil maximum infiltration rate\tnote{*}  &    $19.8$      & \si{\milli\meter\per\hour}\\
      \bottomrule
    \end{tabular}
    \begin{tablenotes}
      \item[*] Parameters spatially distributed.
    \end{tablenotes}
  \end{threeparttable}
\end{table}

%...............................................................................
\subsubsection{Extracting the datasets}
%...............................................................................
This data constitute the \emph{training} dataset for the emulator.
A \emph{test} dataset and a \emph{validation} dataset were also generated and can be found in the Appendix~\ref{Appendix}. \seb{verify if they are really added}

The datasets used to build the emulator was extracted from the simulation outputs.
The extraction was done in two steps.
In the first step the outflow hydrographs were extracted from the output data. 
This was done by summing up the cell discharge along the whole domain bottom boundary for every saved time-step (one value every \SI{60}{\second}).
Result of this operation is a discharge values time series of \num{540} elements.
The same procedure was repeated with every one of the \num{50} experiments.

Fig.~\ref{fig:hydrographs3d} shows the extracted hydrographs.
Here many interesting features can be observed.
It can be seen that experiments run with the two lowest rain intensities and low initial soil saturation generated no discharge.
In the second place, simulations run with $\theta_i = \num{1}$ always reached their peak discharge, and this happens quite quickly.
Some of these show a second smaller increase after the first quick rising limb.
This gets higher the higher the rain intensity.
Some simulations show two "increase bumps" after the first rising limb.
This effect is due to the topography.
The topography used presents zones with very steep slopes and others with very flat ones.
This causes that water droplets at the same distance from the outlet to reach it with different travel times.
When the water from the inclines has reached the outlet, that coming from flat areas is still traveling downstream.
After a given lapse of time this water reaches the outlet as well, giving rise to the bumps visible in the hydrographs.\\

\begin{figure}[h]
  \centering
  \includegraphics[width=0.75\textwidth]{Figures/hydrographs3d.png}
  \caption{Response hydrographs for the \num{50} simulations at the catchment outlet. The red frame shows the end of the rain event. Depth-dimension shows the \emph{rain intensity} variable, while the \emph{initial saturation} variable is rendered by the colormap.}
  \label{fig:hydrographs3d}
\end{figure}

To build the \emph{time-to-threshold emulator}, the emulator giving the arc of time $t_!$ passed from the beginning of the rain event to the exceeding of the threshold discharge $Q_!$, a threshold discharge has to be established.
For a real case study a characteristic discharge value would be set.
This could be the discharge at which a bridge or the embankments downstream of the considered domain are overflown.
Here this value can be chosen with greater freedom, and the effects of choosing different values can be studied.

By looking at the hydrographs of Fig.~\ref{fig:hydrographs3d} it can be observed that $t_!$ is a difficult quantity to predict.
In fact, depending on the $Q_!$ set, there is a variable number of experiments not reaching the threshold, of experiments reaching it very quickly, or of experiment with similar conditions but very different $t_!$.
Especially this last point can be better understood by comparing Fig.~\ref{fig:hydrograph}.
The quantity $t_!$ shows discontinuities.
Within the region defined by the $Q_!$ and the $Q'_!$ lines the hydrograph presents a plateau.
Here, a minimal variation of the threshold set can make the time-to-threshold fall towards $t_!$ or $t'_!$, hence causing a big jump in the value.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/hydrograph.png}
  \caption{Response hydrograph for $\theta_i = \num{0.5}$ and $I = \SI{32.2}{\milli\meter\per\hour}$. The discontinuity in the quantity $t_!$ is visible between the $Q_!$ and the $Q'_!$ lines, where the discharge presents a plateau.}
  \label{fig:hydrograph}
\end{figure}

$Q_!$ was set to \SI{10}{\percent} \seb{why this value? should I give any explanation?} of the $Q_{max}$ value recorded, corresponding to \SI{0.17}{\cubic\meter\per\second}.
The time at which this value is exceeded for the first time was extracted from every hydrograph and forms the dataset for the \emph{regression emulator}.
Another dataset for the \emph{classification emulator} was created by assigning the value \num{-1} to rain events not reaching the threshold and \num{1} to all of the others.

%...............................................................................
\subsubsection{Building-up the classifier}
%...............................................................................
In order to ease the task of computing the time-to-threshold we decided to build a classifier.
The classifier should distinguish whether a rain event is going to reach the threshold discharge or not.
Only the events classified as "reaching the threshold" are given to the emulator to compute when this will happen.
To train the classifier we used the $\num{-1}/\num{1}$ dataset which we extracted in the previous step.
We tested different classifier before finding a satisfying one.
To visually inspect the performance of each different classifier, we plotted the dataset \seb{put reference to plot in the results} in an initial soil saturation vs. rain intensity plot.
Rain events reaching the threshold are marked with red dots, while rain events not reaching it with green dots.
We built the classifier using GP---this with the help of the package \citetalias{rasmussen_gaussian_2010} for \citetalias{octave_community_gnu_2018}.
The package offers a set of mean functions, of covariance functions and likelihood functions as well as prior distribution, inference methods and utility functions that can be used to apply GP.
In addition to the given mean and covariance functions, new functions can be created by composition of others.
New functions can be defined as the sum, the product, the power, etc. of standard ones.
To specify a GP a mean and a covariance function are needed.
The likelihood function defines the probability of the observations given the GP \autocite{rasmussen_gaussian_2010}.
All of these three may have hyperparameters, whose value need to be determined.
Initial values have to be attributed to all of the hyperparameters needed, then the real value must be found by optimization with the function \emph{minimize} of the same package.
The data seemed to be linearly separable, therefore we used the "meanLinear"\seb{should I use inverted commas, italics or what?} mean function.
We combined this with different covariance functions and trained the models with the "minimize" function, by using a "likLogistic" likelihood function and the Expectation Propagation (EP) as inference method.
The use of the "meanLinear" mean function with a "covConst" covariance functions gave good results: the dataset could be linearly separated (cf. Fig~\ref{fig:classifier_trn} in the Appendix). \seb{put this in the appendix or in the results??}
Infinite different curves could define the frontier between events reaching and not reaching the threshold.
After we guessed this first border we added new data points in its proximity by running new simulations.
With these we followed the same procedure explained above to extract their $-1/1$ output.
We added these new observations to the classifier's training dataset and plotted them as "triangles", in order to distinguish them from the previous ones.
The new data points suggested that the boundary is actually not linear; it has a curvy shape instead.
We therefore chose a curvy mean function and combined it with various covariance functions.
After training many models the following set of mean, covariance and likelihood functions combined with the use of EP inference method showed the best results:\\

\inputminted[
  fontsize=\footnotesize,
  firstline=14,
  lastline=22,
  numbersep=2pt,
  gobble=0,
  frame=none,
  bgcolor=light-gray,
  framesep=10mm
]{octave}{code.m}\\

\noindent We trained the model more times since one point could still not be correctly separated and the negative log marginal likelihood could still be further minimized.
We continued until the negative log marginal likelihood could not be further minimized.
The classifier which resulted from this process is the one we took as our final classifier. \seb{could this separate all points correctly??}


%...............................................................................
\subsubsection{Building-up the emulator}
%...............................................................................
The procedure we used to build the time-to-threshold emulator is very similar to the one used for the classifier.
We plotted the \num{50}-points dataset as red dots in a 3D graph, where the x and y axis are given by the rain intensity $I$ and the initial soil saturation $\theta_i$ respectively.
The z-axis shows the response of every rain event, namely the time needed for the rain event to reach the given threshold, which we previously extracted.
We created a $(I, \theta_i)$ grid where to evaluate the emulator:\\

\inputminted[
  fontsize=\footnotesize,
  firstline=30,
  lastline=42,
  numbersep=2pt,
  gobble=0,
  frame=none,
  bgcolor=light-gray,
  framesep=10mm
]{octave}{code.m}\\

\noindent Then we performed a GP regression on the training dataset using the \citetalias{rasmussen_gaussian_2010} package.
As for the classifier, we tested different combinations of mean, covariance and likelihood functions with arbitrary initial hyperparameters values.
As an inference method we used the "infExact" one.
We minimized the negative log marginal likelihood with the "minimize" function, obtained the optimized values for the parameters and evaluated the obtained model on the $(I, \theta_i)$ grid which we previously created.
Since it was sometimes difficult to establish which GP was performing better we created a new dataset, the \emph{test dataset}.
We produced the test dataset the same way we generated the training one, but using different $(I, \theta_i)$ values.
The values of $I$ and $\theta_i$ used can be produced with the code found in Sec.~\ref{sec:test_dataset} of the Appendix.
The test dataset has \num{36} points given by all possible combinations of these values.
We then added the test dataset to the plot and used it to evaluate the performance of the models far from the observations used for the training.
With these points guiding us we established that the model producing the best results was:\\

\inputminted[
  fontsize=\footnotesize,
  firstline=76,
  lastline=84,
  numbersep=2pt,
  gobble=0,
  frame=none,
  bgcolor=light-gray,
  framesep=10mm
]{octave}{code.m}\\

\noindent Finally, we originated a \nth{3} dataset, the \emph{validation dataset}.
This is constituted by \num{6} points sparse in the predictor space.
The values of the $(I, \theta_i)$ pairs composing this dataset can be found in Tab.~\ref{tab:validation_dataset}.
These observation constitute an unbiased dataset, since it was never shown to the model.
We evaluated the performance of our model on both test and validation dataset by computing the MAE and the RMSE.


%-------------------------------------------------------------------------------
\subsection{Results}
%-------------------------------------------------------------------------------
%...............................................................................
\subsubsection{The classifier}\label{sec:classifier}
%...............................................................................
% * Classifier:
%   * show final values of hyperparameters

Fig.~\ref{fig:classifier} displays the results of the classification task.
Green data points correspond to rain events which did not exceed the threshold, while the red ones to those which did.
Circles show the initial dataset (training dataset), whereas "triangles" indicate the dataset which was added in a second time.
According to the classifier every $(I, \theta_i)$ pair chosen in the yellow region will cause the exceeding of $Q_!$.
The classifier could correctly classify all but one points given.
The triangle located at $(I, \theta_i) = (14.7, 0.45)$ is found within the yellow region although its color is green.
The hyperparameters values determined by minimization of the negative log marginal likelihood can be found under Sec.~\ref{sec:classifier_hyperparameters} of the Appendix.


\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/classifier.png}
  \caption{Classifier: the circles represent the first training dataset, while the triangles the second one. In red all events exceeding the threshold, in green all those which did not. Events in the yellow region are the ones classified as "exceeding the threshold", for which the time-to-threshold has to be estimated.}
  \label{fig:classifier}
\end{figure}


%...............................................................................
\subsubsection{The emulator}
%...............................................................................
% Emulator:
%   * show final values of hyperparameters
%   * show MAE, RMSE tables
%   * use emulator with uncertainty in the parameters. Perform UQ

The time-to-threshold emulator is shown in Fig.~\ref{fig:emulator}.
The x and y axis indicate the value of the predictors---rain intensity and soil saturations---while the corresponding response is given on the z-axis.
The circles represent the datasets used for training (red), testing (blue) and validating (green).
The emulator is given by the black mesh.
It is the result of the evaluation of the optimized GP at the \num{6400} evaluation points.
The values of the hyperparameter characterizing this GP emulator can be found under Sec.~\ref{sec:emulator_hyperparameters} in the Appendix.

The performance of the emulator was assessed by computing MAE and RMSE of the validation and test datasets.
These results are summarized in Tab.~\ref{tab:emulator_performance}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{Figures/emulator.png}
  \caption{Time-to-threshold emulator: training (red), test (blue) and validation (green) datasets and the emulator (mesh) predicting the time-to-threshold ($t_!$) in regions where there are no observations.}
  \label{fig:emulator}
\end{figure}


\begin{table}[h]
  \centering
  \caption{Emulator performance on \emph{test} and \emph{validation} datasets.}
  \label{tab:emulator_performance}
  \begin{tabular}{lcc}
    \toprule
     & \textbf{MAE [\si{\minute}, \si{\percent}]} & \textbf{RMSE [\si{\minute}]} \\
    \midrule
    \textbf{test}       & 51.6, 33.6 & 16.1 \\
    \textbf{validation} & 21.8, 11.3 & 10.8 \\
    \bottomrule
  \end{tabular}
\end{table}



%-------------------------------------------------------------------------------
\subsection{Discussion}
%-------------------------------------------------------------------------------
% * emulator should never underestimate danger -> we want to avoid it
%   absolutely)
% * discuss point misclassified
% * discuss influence of time resolution
% * why is test error bigger?
% * if a real system is built -> UQ very important!!

If an early flood warning system based on emulation had to be installed, one would want absolutely to avoid underestimating the danger.
This means, the time to thresholds predicted by the emulator are allowed to be shorter than the real ones, but not longer.
This flood warning system would be the sum of the classifier and the emulator.
Information about the rain event are passed to the classifier (forecasted rain intensity and estimated soil saturation of the catchment).
The classifier would accomplish the first part of the task: assessing if the rain event will reach the given threshold or not.
Under Sec.~\ref{sec:classifier} we saw that the classifier proposed for this catchment and the chosen $Q_!$ fails to classify one point.
In this case the classifiers assigns "reaching threshold" to a point which is actually not reaching it.
In such a scenario, as the one imagined, this error would be quite serious but not dangerous: temporary flood control measures would be prepared, the population would be warned but at the end no flooding would occur.
This would cost money but nobody would be armed.
The misclassified point is a training point.
This means that a better classifier should be found, by further varying the GP used for the classification.
Once a classifier correctly separating all training points is found, its performance should be evaluated.
For this a new dataset, with points close to the determined frontier should be created.
The new dataset should then be classified with the trained classifier and its results should be compared with the actual ones.
The performance would be given by the ration "correctly classified" over "total data points".
If this outcome is satisfactory, then we can keep the classifier obtained.
If not we would add the new dataset to the training dataset, retrain the classifier and retest it on a further new dataset that has to be produced.
This procedure should be repeated until satisfactory results are reached.
It is very important to train a very good classifier.
From this depends if the authorities in charge would take actions or if nothing at all is done.
Consequences, if based on the classifier no action is taken, but the reaching of the threshold does happen, could be dramatical.\\

The emulator, representing the second step of the early flood warning system, even if to a minor extent, should also never underestimate the risk.
The danger linked with underestimating how fast $Q_!$ is reached is somehow a bit smaller, measures are being taken when unexpectedly the flood arrives, but still to consider.











